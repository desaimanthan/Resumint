'use client'

import { useState, useEffect, useRef, useCallback } from 'react'
import { Button } from '@/components/ui/button'
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from '@/components/ui/card'
import { Progress } from '@/components/ui/progress'
import { 
  Mic, 
  MicOff, 
  Volume2, 
  VolumeX, 
  Play, 
  Pause, 
  Square,
  Loader2,
  AlertCircle,
  CheckCircle
} from 'lucide-react'

interface VoiceInterviewAgentProps {
  questions: string[]
  interviewTitle: string
  onTranscriptUpdate: (transcript: string) => void
  onInterviewComplete: (finalTranscript: string) => void
  onError: (error: string) => void
  onInterviewStart?: () => void
  apiCall: (endpoint: string, options?: any) => Promise<any>
}

interface RealtimeEvent {
  type: string
  [key: string]: any
}

export function VoiceInterviewAgent({
  questions,
  interviewTitle,
  onTranscriptUpdate,
  onInterviewComplete,
  onError,
  onInterviewStart,
  apiCall
}: VoiceInterviewAgentProps) {
  const [isConnected, setIsConnected] = useState(false)
  const [isConnecting, setIsConnecting] = useState(false)
  const [isMuted, setIsMuted] = useState(false)
  const [isListening, setIsListening] = useState(false)
  const [currentQuestionIndex, setCurrentQuestionIndex] = useState(0)
  const [transcript, setTranscript] = useState('')
  const [agentSpeaking, setAgentSpeaking] = useState(false)
  const [connectionError, setConnectionError] = useState<string | null>(null)
  
  const wsRef = useRef<WebSocket | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const audioWorkletRef = useRef<AudioWorkletNode | null>(null)
  const audioQueueRef = useRef<string[]>([])
  const isPlayingRef = useRef(false)
  const nextStartTimeRef = useRef(0)

  // Initialize audio context and worklet
  const initializeAudio = useCallback(async () => {
    try {
      // Request microphone permission
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 24000
        } 
      })
      mediaStreamRef.current = stream

      // Create audio context
      audioContextRef.current = new AudioContext({ sampleRate: 24000 })
      
      // Add audio worklet for processing
      await audioContextRef.current.audioWorklet.addModule('/audio-processor.js')
      
      return true
    } catch (error) {
      console.error('Failed to initialize audio:', error)
      onError('Failed to access microphone. Please check permissions.')
      return false
    }
  }, [onError])

  // Connect to OpenAI Realtime API
  const connectToRealtimeAPI = useCallback(async () => {
    if (isConnecting || isConnected) return

    setIsConnecting(true)
    setConnectionError(null)

    try {
      // Get OpenAI API key from backend using apiCall
      const response = await apiCall('/mock-interviews/realtime-token', {
        method: 'POST'
      })

      const { token } = response

      // Connect to OpenAI Realtime API via WebSocket
      const ws = new WebSocket(
        `wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01`,
        ['realtime', `openai-insecure-api-key.${token}`, 'openai-beta.realtime-v1']
      )

      ws.onopen = () => {
        console.log('Connected to OpenAI Realtime API')
        setIsConnected(true)
        setIsConnecting(false)
        
        // Call the interview start callback
        if (onInterviewStart) {
          onInterviewStart()
        }
        
        // Configure the session
        ws.send(JSON.stringify({
          type: 'session.update',
          session: {
            modalities: ['text', 'audio'],
            instructions: `You are a friendly and professional AI interview coach conducting a mock interview. Your voice should be warm, encouraging, and conversational - like talking to a supportive mentor.
            
            Interview Title: ${interviewTitle}
            
            You will ask the following questions one by one:
            ${questions.map((q, i) => `${i + 1}. ${q}`).join('\n')}
            
            Speaking Guidelines:
            - SPEAK EXTREMELY SLOWLY - pause for 3 seconds between each sentence. Take your time with every single word
            - Use a warm, encouraging tone throughout
            - Vary your speech patterns to sound more human-like
            - Include natural conversational elements like "Great!", "I see", "That's interesting"
            - NEVER interrupt the user. Wait at least 5 seconds after they finish speaking before you respond. Speak VERY SLOWLY like you are talking to someone who is hard of hearing
            
            Interview Flow:
            - Start with a warm greeting and brief explanation of the process
            - Ask questions one at a time, allowing full responses
            - Provide encouraging feedback after each answer ("That's a great example", "I appreciate your honesty", etc.)
            - Use transitional phrases between questions ("Let's move on to...", "Now I'd like to ask about...")
            - When complete, thank them warmly and provide encouraging closing remarks
            
            Remember: Sound like a real person having a genuine conversation, not a robot reading a script.`,
            voice: 'shimmer', // Natural female voice (supported by Realtime API)
            input_audio_format: 'pcm16',
            output_audio_format: 'pcm16',
            input_audio_transcription: {
              model: 'whisper-1'
            },
            turn_detection: {
              type: 'server_vad',
              threshold: 0.8,
              prefix_padding_ms: 300,
              silence_duration_ms: 1500
            }
          }
        }))

        // Start the interview
        ws.send(JSON.stringify({
          type: 'conversation.item.create',
          item: {
            type: 'message',
            role: 'user',
            content: [{
              type: 'input_text',
              text: 'Hello! I\'m ready to start my mock interview. Please begin.'
            }]
          }
        }))

        ws.send(JSON.stringify({
          type: 'response.create'
        }))
      }

      ws.onmessage = (event) => {
        const data: RealtimeEvent = JSON.parse(event.data)
        handleRealtimeEvent(data)
      }

      ws.onerror = (error) => {
        console.error('WebSocket error:', error)
        setConnectionError('Connection error occurred')
        setIsConnected(false)
        setIsConnecting(false)
      }

      ws.onclose = () => {
        console.log('WebSocket connection closed')
        setIsConnected(false)
        setIsConnecting(false)
      }

      wsRef.current = ws

    } catch (error) {
      console.error('Failed to connect to Realtime API:', error)
      setConnectionError('Failed to connect to voice service')
      setIsConnecting(false)
    }
  }, [isConnecting, isConnected, questions, interviewTitle])

  // Handle realtime events
  const handleRealtimeEvent = useCallback((event: RealtimeEvent) => {
    switch (event.type) {
      case 'conversation.item.input_audio_transcription.completed':
        // User speech transcribed
        const userText = event.transcript
        setTranscript(prev => prev + `\nUser: ${userText}`)
        onTranscriptUpdate(transcript + `\nUser: ${userText}`)
        break

      case 'response.audio_transcript.delta':
        // AI response being generated
        const aiText = event.delta
        setTranscript(prev => {
          const lines = prev.split('\n')
          const lastLine = lines[lines.length - 1]
          if (lastLine.startsWith('AI: ')) {
            lines[lines.length - 1] = lastLine + aiText
          } else {
            lines.push('AI: ' + aiText)
          }
          return lines.join('\n')
        })
        break

      case 'response.audio_transcript.done':
        // AI response complete
        onTranscriptUpdate(transcript)
        break

      case 'response.audio.delta':
        // Play audio chunk
        if (event.delta) {
          playAudioChunk(event.delta)
        }
        break

      case 'response.done':
        setAgentSpeaking(false)
        break

      case 'input_audio_buffer.speech_started':
        setIsListening(true)
        break

      case 'input_audio_buffer.speech_stopped':
        setIsListening(false)
        break

      case 'error':
        console.error('Realtime API error:', event.error)
        onError(`Voice service error: ${event.error.message}`)
        break
    }
  }, [transcript, onTranscriptUpdate, onError])

  // Play audio chunk
  const playAudioChunk = useCallback((audioData: string) => {
    if (!audioContextRef.current) return

    try {
      // Decode base64 audio data
      const binaryString = atob(audioData)
      const bytes = new Uint8Array(binaryString.length)
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i)
      }

      // Convert to audio buffer and play
      const audioBuffer = audioContextRef.current.createBuffer(1, bytes.length / 2, 24000)
      const channelData = audioBuffer.getChannelData(0)
      
      // Simple PCM16 decoding - let OpenAI handle the quality
      for (let i = 0; i < channelData.length; i++) {
        const sample = (bytes[i * 2] | (bytes[i * 2 + 1] << 8))
        // Convert to signed and normalize
        channelData[i] = sample > 32767 ? (sample - 65536) / 32768 : sample / 32768
      }

      const source = audioContextRef.current.createBufferSource()
      source.buffer = audioBuffer
      source.connect(audioContextRef.current.destination)
      
      source.start()

      setAgentSpeaking(true)
      
      // Clear speaking state when audio ends
      source.onended = () => {
        setAgentSpeaking(false)
      }
    } catch (error) {
      console.error('Failed to play audio chunk:', error)
    }
  }, [])

  // Start interview
  const startInterview = useCallback(async () => {
    const audioInitialized = await initializeAudio()
    if (audioInitialized) {
      await connectToRealtimeAPI()
    }
  }, [initializeAudio, connectToRealtimeAPI])

  // End interview
  const endInterview = useCallback(() => {
    if (wsRef.current) {
      wsRef.current.close()
    }
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop())
    }
    if (audioContextRef.current) {
      audioContextRef.current.close()
    }
    
    onInterviewComplete(transcript)
  }, [transcript, onInterviewComplete])

  // Toggle mute
  const toggleMute = useCallback(() => {
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getAudioTracks().forEach(track => {
        track.enabled = isMuted
      })
      setIsMuted(!isMuted)
    }
  }, [isMuted])

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (wsRef.current) {
        wsRef.current.close()
      }
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop())
      }
      if (audioContextRef.current) {
        audioContextRef.current.close()
      }
    }
  }, [])

  return (
    <div className="space-y-6">
      {/* Connection Status */}
      <Card>
        <CardHeader>
          <CardTitle className="flex items-center gap-2">
            {isConnected ? (
              <CheckCircle className="h-5 w-5 text-green-500" />
            ) : connectionError ? (
              <AlertCircle className="h-5 w-5 text-red-500" />
            ) : (
              <Loader2 className="h-5 w-5 animate-spin" />
            )}
            Voice Interview Agent
          </CardTitle>
          <CardDescription>
            {isConnected 
              ? 'Connected - Ready for voice conversation'
              : isConnecting 
                ? 'Connecting to voice service...'
                : connectionError 
                  ? connectionError
                  : 'Ready to connect'
            }
          </CardDescription>
        </CardHeader>
        <CardContent>
          {!isConnected && !isConnecting && (
            <div className="space-y-4">
              <div className="bg-blue-50 p-4 rounded-lg">
                <h4 className="font-semibold mb-2">Voice Interview Experience:</h4>
                <ul className="text-sm space-y-1 text-muted-foreground">
                  <li>• Have a natural conversation with our AI interviewer</li>
                  <li>• Speak your answers naturally - no typing required</li>
                  <li>• Get real-time feedback and follow-up questions</li>
                  <li>• Complete transcript will be generated automatically</li>
                </ul>
              </div>
              
              <Button onClick={startInterview} size="lg" className="w-full">
                <Play className="h-4 w-4 mr-2" />
                Start Voice Interview
              </Button>
            </div>
          )}

          {isConnected && (
            <div className="space-y-4">
              {/* Audio Controls */}
              <div className="flex items-center justify-center gap-4">
                <Button
                  variant={isMuted ? "destructive" : "outline"}
                  size="lg"
                  onClick={toggleMute}
                >
                  {isMuted ? <MicOff className="h-5 w-5" /> : <Mic className="h-5 w-5" />}
                  {isMuted ? 'Unmute' : 'Mute'}
                </Button>

                <div className="flex items-center gap-2">
                  {isListening && (
                    <div className="flex items-center gap-2 text-green-600">
                      <div className="w-2 h-2 bg-green-500 rounded-full animate-pulse"></div>
                      <span className="text-sm">Listening...</span>
                    </div>
                  )}
                  {agentSpeaking && (
                    <div className="flex items-center gap-2 text-blue-600">
                      <Volume2 className="h-4 w-4" />
                      <span className="text-sm">AI Speaking...</span>
                    </div>
                  )}
                </div>

                <Button
                  variant="destructive"
                  size="lg"
                  onClick={endInterview}
                >
                  <Square className="h-5 w-5 mr-2" />
                  End Interview
                </Button>
              </div>

              {/* Progress */}
              <div className="space-y-2">
                <div className="flex justify-between text-sm">
                  <span>Interview Progress</span>
                  <span>{Math.round((currentQuestionIndex / questions.length) * 100)}% Complete</span>
                </div>
                <Progress value={(currentQuestionIndex / questions.length) * 100} />
              </div>
            </div>
          )}
        </CardContent>
      </Card>

      {/* Live Transcript */}
      {isConnected && transcript && (
        <Card>
          <CardHeader>
            <CardTitle>Live Transcript</CardTitle>
            <CardDescription>
              Real-time conversation transcript
            </CardDescription>
          </CardHeader>
          <CardContent>
            <div className="bg-gray-50 p-4 rounded-lg max-h-64 overflow-y-auto">
              <pre className="whitespace-pre-wrap text-sm leading-relaxed">
                {transcript}
              </pre>
            </div>
          </CardContent>
        </Card>
      )}
    </div>
  )
}
